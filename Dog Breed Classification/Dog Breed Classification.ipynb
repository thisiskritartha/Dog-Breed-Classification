{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fba2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a20b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0732469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680da9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb1b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14388 images belonging to 74 classes.\n",
      "Found 3566 images belonging to 74 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir, \n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset = 'training'\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(\n",
    "base_dir,\n",
    "target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fe8225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Afghan': 0, 'African_wild_dog': 1, 'Airedale': 2, 'American Hairless': 3, 'American Spaniel': 4, 'Basenji': 5, 'Basset': 6, 'Beagle': 7, 'Bearded collie': 8, 'Bermaise': 9, 'Bichon Frise': 10, 'Blenheim': 11, 'Bloodhound': 12, 'Bluetick': 13, 'Border collie': 14, 'Borzoi': 15, 'Boston Terrier': 16, 'Boxer': 17, 'Bull Mastiff': 18, 'Bull Terrier': 19, 'Bulldog': 20, 'Cairn': 21, 'Chihuahua': 22, 'Chinese Crested': 23, 'Chow': 24, 'Clumber': 25, 'Cockapoo': 26, 'Cocker': 27, 'Collie': 28, 'Corgi': 29, 'Coyote': 30, 'Dalmation': 31, 'Dhole': 32, 'Dingo': 33, 'Doberman': 34, 'Elk hound': 35, 'French bulldog': 36, 'German Sheperd': 37, 'Golden Retriever': 38, 'Great Dane': 39, 'Great Perenees': 40, 'Greyhound': 41, 'Groenendael': 42, 'Himalayan Sheepdog': 43, 'Human Beings': 44, 'Irish Spaniel': 45, 'Irish Wolfhound': 46, 'Japanese Spaniel': 47, 'Japanese Spitz': 48, 'Komondor': 49, 'Labradoodle': 50, 'Labrador': 51, 'Lhasa': 52, 'Malinois': 53, 'Maltese': 54, 'Mexican Hairless': 55, 'Newfoundland': 56, 'Other': 57, 'Pekinese': 58, 'Pit Bull': 59, 'Pomeranian': 60, 'Poodle': 61, 'Pug': 62, 'Rhodesian': 63, 'Rottweiler': 64, 'Saint Bernard': 65, 'Schnauzer': 66, 'Scotch Terrier': 67, 'Shar_Pei': 68, 'Shiba Inu': 69, 'Shih-Tzu': 70, 'Siberian Husky': 71, 'Vizsla': 72, 'Yorkshire Terrier': 73}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "with open('labels.txt', 'w') as f:\n",
    "    f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a104e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape = IMG_SHAPE,\n",
    "    include_top = False,\n",
    "    weights = 'imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38d69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Conv2D(32, 3,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0, 2),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(74,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4a3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cccaa58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "225/225 [==============================] - 952s 4s/step - loss: 1.6346 - accuracy: 0.5963 - val_loss: 0.6812 - val_accuracy: 0.8197\n",
      "Epoch 2/35\n",
      "225/225 [==============================] - 667s 3s/step - loss: 0.6270 - accuracy: 0.8189 - val_loss: 0.5837 - val_accuracy: 0.8396\n",
      "Epoch 3/35\n",
      "225/225 [==============================] - 661s 3s/step - loss: 0.4401 - accuracy: 0.8579 - val_loss: 0.5753 - val_accuracy: 0.8421\n",
      "Epoch 4/35\n",
      "225/225 [==============================] - 2464s 11s/step - loss: 0.3243 - accuracy: 0.8910 - val_loss: 0.5523 - val_accuracy: 0.8559\n",
      "Epoch 5/35\n",
      "225/225 [==============================] - 309s 1s/step - loss: 0.2552 - accuracy: 0.9103 - val_loss: 0.5496 - val_accuracy: 0.8702\n",
      "Epoch 6/35\n",
      "225/225 [==============================] - 312s 1s/step - loss: 0.2028 - accuracy: 0.9265 - val_loss: 0.5307 - val_accuracy: 0.8718\n",
      "Epoch 7/35\n",
      "225/225 [==============================] - 317s 1s/step - loss: 0.1536 - accuracy: 0.9440 - val_loss: 0.5483 - val_accuracy: 0.8632\n",
      "Epoch 8/35\n",
      "225/225 [==============================] - 333s 1s/step - loss: 0.1344 - accuracy: 0.9488 - val_loss: 0.6152 - val_accuracy: 0.8648\n",
      "Epoch 9/35\n",
      "225/225 [==============================] - 343s 2s/step - loss: 0.1133 - accuracy: 0.9554 - val_loss: 0.5959 - val_accuracy: 0.8724\n",
      "Epoch 10/35\n",
      "225/225 [==============================] - 344s 2s/step - loss: 0.1094 - accuracy: 0.9568 - val_loss: 0.6146 - val_accuracy: 0.8704\n",
      "Epoch 11/35\n",
      "225/225 [==============================] - 341s 2s/step - loss: 0.1116 - accuracy: 0.9546 - val_loss: 0.6528 - val_accuracy: 0.8615\n",
      "Epoch 12/35\n",
      "225/225 [==============================] - 344s 2s/step - loss: 0.0922 - accuracy: 0.9618 - val_loss: 0.6817 - val_accuracy: 0.8651\n",
      "Epoch 13/35\n",
      "225/225 [==============================] - 343s 2s/step - loss: 0.0758 - accuracy: 0.9670 - val_loss: 0.6346 - val_accuracy: 0.8682\n",
      "Epoch 14/35\n",
      "225/225 [==============================] - 344s 2s/step - loss: 0.0657 - accuracy: 0.9698 - val_loss: 0.7101 - val_accuracy: 0.8668\n",
      "Epoch 15/35\n",
      "225/225 [==============================] - 556s 2s/step - loss: 0.0623 - accuracy: 0.9704 - val_loss: 0.6732 - val_accuracy: 0.8744\n",
      "Epoch 16/35\n",
      "225/225 [==============================] - 477s 2s/step - loss: 0.0885 - accuracy: 0.9606 - val_loss: 0.7826 - val_accuracy: 0.8511\n",
      "Epoch 17/35\n",
      "225/225 [==============================] - 400s 2s/step - loss: 0.1376 - accuracy: 0.9438 - val_loss: 0.7845 - val_accuracy: 0.8536\n",
      "Epoch 18/35\n",
      "225/225 [==============================] - 411s 2s/step - loss: 0.1082 - accuracy: 0.9536 - val_loss: 0.7628 - val_accuracy: 0.8738\n",
      "Epoch 19/35\n",
      "225/225 [==============================] - 416s 2s/step - loss: 0.0765 - accuracy: 0.9653 - val_loss: 0.7428 - val_accuracy: 0.8617\n",
      "Epoch 20/35\n",
      "225/225 [==============================] - 391s 2s/step - loss: 0.0614 - accuracy: 0.9699 - val_loss: 0.7902 - val_accuracy: 0.8612\n",
      "Epoch 21/35\n",
      "225/225 [==============================] - 393s 2s/step - loss: 0.0524 - accuracy: 0.9739 - val_loss: 0.7887 - val_accuracy: 0.8632\n",
      "Epoch 22/35\n",
      "225/225 [==============================] - 392s 2s/step - loss: 0.0450 - accuracy: 0.9764 - val_loss: 0.7650 - val_accuracy: 0.8662\n",
      "Epoch 23/35\n",
      "225/225 [==============================] - 402s 2s/step - loss: 0.0454 - accuracy: 0.9732 - val_loss: 0.7348 - val_accuracy: 0.8794\n",
      "Epoch 24/35\n",
      "225/225 [==============================] - 386s 2s/step - loss: 0.0414 - accuracy: 0.9771 - val_loss: 0.7896 - val_accuracy: 0.8749\n",
      "Epoch 25/35\n",
      "225/225 [==============================] - 383s 2s/step - loss: 0.0426 - accuracy: 0.9761 - val_loss: 0.7857 - val_accuracy: 0.8704\n",
      "Epoch 26/35\n",
      "225/225 [==============================] - 377s 2s/step - loss: 0.0712 - accuracy: 0.9659 - val_loss: 0.9700 - val_accuracy: 0.8427\n",
      "Epoch 27/35\n",
      "225/225 [==============================] - 366s 2s/step - loss: 0.2199 - accuracy: 0.9228 - val_loss: 0.9261 - val_accuracy: 0.8500\n",
      "Epoch 28/35\n",
      "225/225 [==============================] - 379s 2s/step - loss: 0.1267 - accuracy: 0.9499 - val_loss: 0.9092 - val_accuracy: 0.8657\n",
      "Epoch 29/35\n",
      "225/225 [==============================] - 388s 2s/step - loss: 0.0632 - accuracy: 0.9683 - val_loss: 0.9228 - val_accuracy: 0.8699\n",
      "Epoch 30/35\n",
      "225/225 [==============================] - 386s 2s/step - loss: 0.0488 - accuracy: 0.9742 - val_loss: 0.8782 - val_accuracy: 0.8766\n",
      "Epoch 31/35\n",
      "225/225 [==============================] - 572s 3s/step - loss: 0.0407 - accuracy: 0.9759 - val_loss: 0.8598 - val_accuracy: 0.8825\n",
      "Epoch 32/35\n",
      "225/225 [==============================] - 672s 3s/step - loss: 0.0407 - accuracy: 0.9771 - val_loss: 0.8452 - val_accuracy: 0.8783\n",
      "Epoch 33/35\n",
      "225/225 [==============================] - 544s 2s/step - loss: 0.0409 - accuracy: 0.9762 - val_loss: 0.8681 - val_accuracy: 0.8777\n",
      "Epoch 34/35\n",
      "225/225 [==============================] - 521s 2s/step - loss: 0.0396 - accuracy: 0.9760 - val_loss: 0.8848 - val_accuracy: 0.8780\n",
      "Epoch 35/35\n",
      "225/225 [==============================] - 363s 2s/step - loss: 0.0378 - accuracy: 0.9766 - val_loss: 0.8339 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "epochs = 35\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a39c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_file = 'predict.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e97bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3d63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(keras_file)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70d8ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\its\\AppData\\Local\\Temp\\tmpr8z4ou7w\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\its\\AppData\\Local\\Temp\\tmpr8z4ou7w\\assets\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b00049f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10351048"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('model.tflite','wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01acf62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
